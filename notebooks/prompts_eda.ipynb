{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompts Exploratory Data Analysis\n",
    "Using this notebook, we can analyze and extract key insights from the prompts used for training our LLM model.\n",
    "\n",
    "#### EDA Overview\n",
    "Token Wise:\n",
    "- Average Tokens Length\n",
    "- Token Lengths Distribution\n",
    "- Min/Max in tokens\n",
    "\n",
    "Coverage:\n",
    "- Prompt Coverage as a function of Tokens Lenght\n",
    "- Prompt Coverage as a function of hardcoded LLM context size.\n",
    "\n",
    "Word Analysis:\n",
    "- Word Cloud\n",
    "---\n",
    "\n",
    "Insights:\n",
    "- `Average Tokens Length` : see wether our prompts are balanced in relation to model context size.\n",
    "- `Token Lengths Distribution` : see variety of sentence lengths, skewed towards short or long sentences\n",
    "- `Min/Max in tokens` : anomalies or outliers in the training set\n",
    "\n",
    "- `Prompt Coverage w.r.t Tokens Lenght` : how effective the prompts cover the entire range of tokens lengths\n",
    "- `Word Cloud` : main topics and subjects within the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Dataset Prerequisites\n",
    "# 1. Load dataset, process each prompt such that it is a raw string.\n",
    "# 2. Load tokenizer, set SOL, EOS\n",
    "# 3. Add SOL/EOS to each prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt [1] Average Tokens Length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenize each prompt\n",
    "# Gather token lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt [2] Token Lengths Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Plot tokens distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt [3] Min/Max in Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Plot percentiles .25 .5 .75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coverage [1] Prompt Coverage as a function of Tokens Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Plot gaussian bell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coverage [2] Prompt Coverage as a function of hardcoded context size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Plot prompts coverage in relation to tokens count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Analysis [1] World Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Plot word cloud distribution"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
